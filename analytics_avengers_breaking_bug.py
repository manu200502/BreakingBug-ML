# -*- coding: utf-8 -*-
"""Analytics Avengers_Breaking Bug.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19CELt_OvbN0gU_Om9DR1Nlm8bUTPZs6A
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.metrics import accuracy_score

df = pd.read_csv('dataset (1).csv')

# Basic Data Exploration
print(df['id'].max())
print(df['age'].min(), df['age'].max())
print(df['age'].describe())

print('Mean:', df['age'].mean())
print('Median:', df['age'].median())
print('Mode:', df['age'].mode()[0])

# Age distribution plot
sns.histplot(df['age'], kde=True, color="#FF5733")
plt.axvline(df['age'].mean(), color='Red', linestyle='--')
plt.axvline(df['age'].median(), color='Green', linestyle='--')
plt.axvline(df['age'].mode()[0], color='Blue', linestyle='--')
plt.show()

# Age distribution by sex
fig = px.histogram(data_frame=df, x='age', color='sex')
fig.show()

# Sex distribution
sex_counts = df['sex'].value_counts()
male_count = sex_counts.get('Male', 0)
female_count = sex_counts.get('Female', 0)

total_count = male_count + female_count
male_percentage = (male_count / total_count) * 100
female_percentage = (female_count / total_count) * 100

print(f'Male percentage in the data: {male_percentage:.2f}%')
print(f'Female percentage in the data: {female_percentage:.2f}%')
difference_percentage = ((male_count - female_count) / female_count) * 100
print(f'Males are {difference_percentage:.2f}% more than females in the data.')

# Group by sex and age
print(df.groupby('sex')['age'].value_counts())

# Dataset column unique values
print(df['dataset'].value_counts())

# Plot dataset count by sex
fig = px.bar(df, x='dataset', color='sex')
fig.show()

print(df.groupby('sex')['dataset'].value_counts())

# Data Preprocessing
df = df.dropna()  # Dropping rows with missing values for simplicity
X = df.drop(columns=['num', 'id'])
y = df['num']

# Encode categorical variables
X = pd.get_dummies(X, drop_first=True)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def hyperparameter_tuning(X_train, y_train, X_test, y_test, models):
    results = {}
    for model_name, model in models.items():
        if model_name == 'Logistic Regression':
            param_grid = {'C': [0.01, 0.1, 1, 10, 100]}
        elif model_name == 'KNN':
            param_grid = {'n_neighbors': [3, 5, 7, 9]}
        elif model_name == 'NB':
            param_grid = {}  # No hyperparameters to tune for GaussianNB
        elif model_name == 'SVM':
            param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100]}
        elif model_name == 'Decision Tree':
            param_grid = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}
        elif model_name == 'Random Forest':
            param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}
        elif model_name == 'XGBoost':
            param_grid = {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 7]}
        elif model_name == 'GradientBoosting':
            param_grid = {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200, 300], 'max_depth': [3, 5, 7]}
        elif model_name == 'AdaBoost':
            param_grid = {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [50, 100, 200]}

        # Perform hyperparameter tuning using GridSearchCV
        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')
        grid_search.fit(X_train, y_train)

        # Get best hyperparameters and evaluate on train and test sets
        best_params = grid_search.best_params_
        best_model = grid_search.best_estimator_
        y_train_pred = best_model.predict(X_train)
        y_test_pred = best_model.predict(X_test)
        train_accuracy = accuracy_score(y_train, y_train_pred)
        test_accuracy = accuracy_score(y_test, y_test_pred)

        # Store results in dictionary
        results[model_name] = {
            'best_params': best_params,
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy
        }

    return results

# Define models dictionary
models = {
    "Logistic Regression": LogisticRegression(),
    "KNN": KNeighborsClassifier(),
    "NB": GaussianNB(),
    "SVM": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": GradientBoostingClassifier(),  # Assuming XGBoost is not installed, using GradientBoostingClassifier as placeholder
    "GradientBoosting": GradientBoostingClassifier(),
    "AdaBoost": AdaBoostClassifier()
}

# Example usage:
results = hyperparameter_tuning(X_train, y_train, X_test, y_test, models)
for model_name, result in results.items():
    print("Model:", model_name)
    print("Best hyperparameters:", result['best_params'])
    print("Train Accuracy:", result['train_accuracy'])
    print("Test Accuracy:", result['test_accuracy'])
    print()